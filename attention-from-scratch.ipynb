{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#the logic is \n#take the input make the q k v things\n# break them down (by dimention of embedding // nb heads)\n#apply the fomua of attention thing \n#concat","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport math\nimport torch.nn.functional as F\n\nclass MultiheadAttention(nn.Module):\n    def __init__(self,batch_size=1,seq_length=10,nb_heads=8,d_model=512,dropout=0.1):\n        super(MultiheadAttention, self).__init__()\n        assert d_model% nb_heads==0\n        self.d_model = d_model\n        self.nb_heads=nb_heads\n        self.d_k = d_model // nb_heads\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_forward = nn.Linear(d_model, d_model) #for combind gthe outputs later\n        #in here we just defined all we would need we did not\n        #resize/rescale anything /mul anything yet we\n        # do all that mess in foirward\n    def scaled_dot_prod(self,q,v,k,d_k):\n        #the fc that does the main logic idea\n        #that i know the math but just gonna copy how they write it\n        scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n        attention = F.softmax(scaled, dim=-1)\n        values = torch.matmul(attention, v)\n        return values\n    def pass_heads(self,x):\n        #now pass that shit through the heads\n        #this function changes teh struc of the treatment so\n        #it is on multiple heads instead of one\n        #\"one big attention\" to \"multiple small heads.\"\n        batch_size, seq_length, d_model = x.size()\n        return x.view(batch_size, seq_length, self.nb_heads, self.d_k).transpose(1, 2)\n    def combine_heads(self,x):\n        batch_size, num_heads, seq_length, d_k = x.size()\n        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n    def forward(self,q,k,v):\n        Q = self.pass_heads(self.W_q(q))\n        K = self.pass_heads(self.W_k(k))\n        V = self.pass_heads(self.W_v(v))\n        print(Q.shape)\n        #what this do is change the dimensions of the q,k,v\n     #they are no longer working with single head they have many now\n        attn_output = self.scaled_dot_prod(Q, K, V, self.d_k)\n        output = self.W_forward(self.combine_heads(attn_output))\n        return output\n\n        \n        \n        \n    \n        \n        \n        \n        \n        \n        \n        \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T16:53:29.416542Z","iopub.execute_input":"2025-06-07T16:53:29.416906Z","iopub.status.idle":"2025-06-07T16:53:29.428251Z","shell.execute_reply.started":"2025-06-07T16:53:29.416882Z","shell.execute_reply":"2025-06-07T16:53:29.427029Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"d_model = 8\nnum_heads = 4\nquery_seq_length = 5\nke_seq_length = 6\nbatch_size = 1\n\nmha = MultiheadAttention(batch_size,query_seq_length, num_heads,d_model)\n\nQ = torch.randn(batch_size, query_seq_length, d_model)\nK =  V = torch.randn(batch_size, ke_seq_length, d_model)\n \nmask = torch.ones(batch_size, 1, query_seq_length, ke_seq_length)\n\n# Forward pass\noutput = mha(Q, K, V)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T16:53:34.480129Z","iopub.execute_input":"2025-06-07T16:53:34.480431Z","iopub.status.idle":"2025-06-07T16:53:34.493717Z","shell.execute_reply.started":"2025-06-07T16:53:34.480409Z","shell.execute_reply":"2025-06-07T16:53:34.492702Z"}},"outputs":[{"name":"stdout","text":"torch.Size([1, 4, 5, 2])\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"print(output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T16:50:28.552655Z","iopub.execute_input":"2025-06-07T16:50:28.553003Z","iopub.status.idle":"2025-06-07T16:50:28.610122Z","shell.execute_reply.started":"2025-06-07T16:50:28.552960Z","shell.execute_reply":"2025-06-07T16:50:28.608596Z"}},"outputs":[{"name":"stdout","text":"tensor([[[-0.1781, -0.2942,  0.2713,  0.1556, -0.0173, -0.1651, -0.3160,\n           0.0702],\n         [-0.2155, -0.3244,  0.2264,  0.2289,  0.0026, -0.1848, -0.2680,\n           0.0952],\n         [-0.2071, -0.2984,  0.2440,  0.1849, -0.0024, -0.1802, -0.2867,\n           0.0799],\n         [-0.1839, -0.2773,  0.2044,  0.2137,  0.0723, -0.1786, -0.3495,\n           0.0462],\n         [-0.1440, -0.2833,  0.2329,  0.1591,  0.0198, -0.1609, -0.3903,\n           0.0553]]], grad_fn=<ViewBackward0>)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
