{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12548109,"sourceType":"datasetVersion","datasetId":7922608}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## What is CLIP ?\n","metadata":{}},{"cell_type":"markdown","source":"CLIP : Contrastive Language–Image Pretraining, it is a model that knows how to match between text and images , the role of clip is not to generate new conten but to match on a semantic metric the image and text we pass as input ,it is a zeroshot classifier it can be used for \nClassification\nSemantic understanding ,\nLabeling pictures\nIn the following notebook i will try to work on implementing CLIP \"from scratch\" to some extend but my interest is in understanding how the constrative pretraining work , i will try to implement the InfoNCE loss and answer questions that i thought of when working with this model\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import ViTModel, ViTFeatureExtractor, AutoTokenizer, AutoModel\nfrom PIL import Image\nimport torch.nn.functional as F\nclass ContrastiveLearningModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.device = \"cpu\"\n        # image mdoel to get image embedding \n        self.imagemodel = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n        self.feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n        self.imagemodel.eval()\n\n        # text model to get text embedding\n        model_name = \"bert-base-uncased\"\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.textmodel = AutoModel.from_pretrained(model_name)\n        self.textmodel.eval()\n\n        # Projection layers here the dimension size is 256\n        projection_dim = 256\n        self.image_proj = nn.Linear(self.imagemodel.config.hidden_size, projection_dim)# (B,256)\n        self.text_proj = nn.Linear(self.textmodel.config.hidden_size, projection_dim) #(B,256)\n\n    def forward(self, image, text,temp=0.07):\n        # Process text\n        text_inputs = self.tokenizer(text, return_tensors=\"pt\")\n        with torch.no_grad():\n            text_outputs = self.textmodel(**text_inputs)\n        text_embedding = text_outputs.last_hidden_state[:, 0, :]  \n\n        # Process image\n        image_inputs = self.feature_extractor(images=image, return_tensors=\"pt\")\n        with torch.no_grad():\n            image_outputs = self.imagemodel(**image_inputs)\n        image_embedding = image_outputs.last_hidden_state[:, 0, :]  \n\n        # Project to joint space\n        image_proj = self.image_proj(image_embedding)  # shape: (1, projection_dim)  (batch size, projection dim)\n        text_proj = self.text_proj(text_embedding)     # shape: (1, projection_dim)\n\n        # Normalize\n        image_proj = nn.functional.normalize(image_proj, dim=-1)\n        text_proj = nn.functional.normalize(text_proj, dim=-1)\n        #get simliairy\n        logits=image_proj@text_proj.T  #(B,B)\n        logits/=temp\n        batch_size=image_proj.shape[0]\n        labels=torch.arange(logits.shape[0], device=logits.device)\n\n         #cross entropy\n\n        loss_I = F.cross_entropy(logits.T, labels)\n        loss_T = F.cross_entropy(logits, labels)\n        loss = (loss_I + loss_T)/2.0 \n\n        return loss,logits\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T23:29:54.132847Z","iopub.execute_input":"2025-07-28T23:29:54.133365Z","iopub.status.idle":"2025-07-28T23:30:31.674529Z","shell.execute_reply.started":"2025-07-28T23:29:54.133329Z","shell.execute_reply":"2025-07-28T23:30:31.673380Z"}},"outputs":[{"name":"stderr","text":"2025-07-28 23:30:14.147211: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753745414.414585      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753745414.495361      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# How the flow goes \n\nWe get text and image embeddings from the text and image encoder (can be Vit or resnet)\nWe project both embeddings in the shared dimension space using Wi and Wt the matrixes of dimensions (B,dimension_size) both\nYour raw features (img_feat, text_feat) are not guaranteed to have any alignment — even if ViT and BERT are pretrained!\n\nWe normalize both vectors\ncause what we care about is what direction the embedding vecotr is going in , not how long ,big it is\n\n\nFinally we Compute the similarity between both dimensions using dot product (textproj@imageproj) \nThis simliairty matrix would be of dimension (B,B) the magic is that the datapoints on the diagonal would be the ones having the correct matching aka the highest simliarity score\n\nDuring training\n\nMeasures cosine similarity of image-text pairs\n\nApplies InfoNCE loss to push matching pairs together and others apart\n\nGradient updates adjust the projection weights to better align matches\n","metadata":{}},{"cell_type":"markdown","source":"# Why the data poitns on diagonal have same simliary score   \n//imporve later\n\nduring the training , the model learns to push together models having same semantic meaning , pushing far away ones with differen semantic meaning , this is cause of the infonce loss","metadata":{}},{"cell_type":"markdown","source":"# what are we training here?\n\n what will be changing during training is the projection matrices wi and wt those learn neeed to learn to project the image and text of same semantic meaning in the same direction that way during matrix representation they both be on the diagnoanl?\n The weights of the projection layers learn how to map features into a space where semantic relationships are reflected geometrically (through vector direction and distance).\n\n The projection weights are trained (via InfoNCE loss) to rotate/scale/transform the original features in a way that:\n\ndog image ↔ \"a dog\" → vectors point in the same direction\n\ncat image ↔ \"a cat\" → same\n\ndog image ↔ \"a cat\" → vectors point in different directions\n\n","metadata":{}},{"cell_type":"markdown","source":"resources \nhttps://medium.com/@paluchasz/understanding-openais-clip-model-6b52bade3fa3\n","metadata":{}},{"cell_type":"markdown","source":"# Implementation","metadata":{}},{"cell_type":"code","source":"# === Inference ===\nclip = ContrastiveLearningModel()\nclip.to(clip.device)\ntext = [\"a cat\"]  # list of text\nimage = Image.open('/kaggle/input/catttt/prettyboy.jpg').convert('RGB')  # single image\n\nloss, logits = clip.forward(image, text)\nprint(\"Loss:\", loss.item())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T23:30:31.679014Z","iopub.execute_input":"2025-07-28T23:30:31.679279Z","iopub.status.idle":"2025-07-28T23:30:46.635632Z","shell.execute_reply.started":"2025-07-28T23:30:31.679259Z","shell.execute_reply":"2025-07-28T23:30:46.634491Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1268dff271a3474baaa2f96ece4792ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d942e7d96e1c44299b41e271aba6347e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c65773a4a49494cbf15b096a6eec41c"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/vit/feature_extraction_vit.py:30: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b32a6791c744047be0bf09eb5ea1b62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"150c35dca89e4669bd5577f3f3c97383"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4cd23a6643c4734a81af567470214e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e77a6ca213294791a766ca1c7fedcf75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7efc94591344224a54cf96b920889ed"}},"metadata":{}},{"name":"stdout","text":"Loss: 0.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Impltted clip\n","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nimport requests\n\nfrom transformers import CLIPProcessor, CLIPModel\n\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T23:30:46.637537Z","iopub.execute_input":"2025-07-28T23:30:46.637932Z","iopub.status.idle":"2025-07-28T23:30:56.051824Z","shell.execute_reply.started":"2025-07-28T23:30:46.637900Z","shell.execute_reply":"2025-07-28T23:30:56.050622Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d4f664efe7e4d04bfa5979a20dd6558"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6877135b5eb548bebaa9c9f55ae72bf0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b82f9d55fa64d6984da04a095566dfb"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d21b07e187c44e45a11e8b7c892884c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ae52a8eee694abe9951982602d10e93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b7046afd3614606ab85be040241ac66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56a4701637f544e6aa6890839514cacf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1f89b60b0a645d8aec0c0436ee25322"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57db22c659e84b839bd5029029e100cd"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"\nurl = \"/kaggle/input/catttt/prettyboy.jpg\"\nimage = Image.open(url)\n\n\ninputs = processor(text=[\"a photo of a lion\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image # this is the image-text similarity score\nprobs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}